{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a5f5536",
   "metadata": {},
   "source": [
    "# 03 — Listing-Level Demand Forecasting (Spark MLlib / Big-Data Strategy)\n",
    "\n",
    "This notebook trains **listing-level demand models** directly in Spark using the engineered dataset from Notebook 02.\n",
    "\n",
    "**Data source (single source of truth):**\n",
    "- `data/processed/listing_features.parquet` (includes `elasticity_slope` already added in Notebook 02)\n",
    "\n",
    "**Goal:** predict **booking demand** (`n_bookings`) using listing + pricing + market context features (including `elasticity_slope`).\n",
    "\n",
    "**Approach (Spark-native):**\n",
    "- Load Parquet into a Spark DataFrame (no pandas conversion)\n",
    "- Build a Spark MLlib pipeline: type cleaning → imputation → vector assembly → model\n",
    "- Train and evaluate multiple regressors (baseline + tree models)\n",
    "- Save the best model and feature importances (where supported)\n",
    "\n",
    "**Outputs**\n",
    "- `outputs/spark_model_results.csv`\n",
    "- `outputs/spark_feature_importance.csv` (tree models)\n",
    "- `models/spark_best_model/` (Spark MLlib pipeline model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fd7bbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b91d476a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Locate project root (folder containing 'src')\n",
    "project_root = Path.cwd()\n",
    "for p in [project_root] + list(project_root.parents):\n",
    "    if (p / 'src').exists():\n",
    "        project_root = p\n",
    "        break\n",
    "if not (project_root / 'src').exists():\n",
    "    raise FileNotFoundError(\"Could not find 'src' directory.\")\n",
    "\n",
    "# Windows Spark/Hadoop helpers (assumes hadoop binaries already exist from Notebook 01)\n",
    "hadoop_home = project_root / 'hadoop'\n",
    "bin_dir = hadoop_home / 'bin'\n",
    "os.environ['HADOOP_HOME'] = str(hadoop_home)\n",
    "os.environ['hadoop.home.dir'] = str(hadoop_home)\n",
    "os.environ['PATH'] = str(bin_dir) + os.pathsep + os.environ.get('PATH', '')\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName('airbnb-demand-forecasting-sparkml')\n",
    "    .master('local[*]')\n",
    "    # Stability / memory headroom for ML on Windows\n",
    "    .config('spark.driver.memory', '4g')\n",
    "    .config('spark.sql.shuffle.partitions', '64')\n",
    "    .config('spark.default.parallelism', '64')\n",
    "    .getOrCreate()\n",
    " )\n",
    "\n",
    "# Keep Spark execution stable on Windows\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "\n",
    "print('Spark version:', spark.version)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d9177c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist datasets (Spark writes)\n",
    "out_processed = project_root / 'data' / 'processed'\n",
    "out_processed.mkdir(parents=True, exist_ok=True)\n",
    "outputs_dir = project_root / 'outputs'\n",
    "outputs_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bdf9880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing features path: data/processed/listing_features.parquet\n",
      "File exists: True\n"
     ]
    }
   ],
   "source": [
    "# Verify listing features file exists\n",
    "listing_features_file = out_processed / 'listing_features.parquet'\n",
    "print(f\"Listing features path: data/processed/listing_features.parquet\")\n",
    "\n",
    "if listing_features_file.exists():\n",
    "    print(f\"File exists: {listing_features_file.exists()}\")\n",
    "else:\n",
    "    print(\"File not found. Run Notebook 02 to generate listing_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee5a1dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded listing_features.parquet with 49 columns\n",
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- weekend: integer (nullable = true)\n",
      " |-- cat_city_amsterdam: integer (nullable = true)\n",
      " |-- cat_city_athens: integer (nullable = true)\n",
      " |-- cat_city_barcelona: integer (nullable = true)\n",
      " |-- cat_city_berlin: integer (nullable = true)\n",
      " |-- cat_city_budapest: integer (nullable = true)\n",
      " |-- cat_city_lisbon: integer (nullable = true)\n",
      " |-- cat_city_london: integer (nullable = true)\n",
      " |-- cat_city_paris: integer (nullable = true)\n",
      " |-- cat_city_rome: integer (nullable = true)\n",
      " |-- cat_city_vienna: integer (nullable = true)\n",
      " |-- is_weekend: integer (nullable = true)\n",
      " |-- listing_price: double (nullable = true)\n",
      " |-- room_shared: integer (nullable = true)\n",
      " |-- room_private: integer (nullable = true)\n",
      " |-- person_capacity: double (nullable = true)\n",
      " |-- host_is_superhost: integer (nullable = true)\n",
      " |-- multi: integer (nullable = true)\n",
      " |-- biz: integer (nullable = true)\n",
      " |-- cleanliness_rating: double (nullable = true)\n",
      " |-- guest_satisfaction_overall: double (nullable = true)\n",
      " |-- bedrooms: double (nullable = true)\n",
      " |-- city_center_dist: double (nullable = true)\n",
      " |-- metro_dist: double (nullable = true)\n",
      " |-- n_bookings: integer (nullable = true)\n",
      " |-- cat_room_type_entire_home_apt: integer (nullable = true)\n",
      " |-- cat_room_type_private_room: integer (nullable = true)\n",
      " |-- cat_room_type_shared_room: integer (nullable = true)\n",
      " |-- log_price: double (nullable = true)\n",
      " |-- price_per_person: double (nullable = true)\n",
      " |-- price_per_bedroom: double (nullable = true)\n",
      " |-- capacity_bin: string (nullable = true)\n",
      " |-- quality_score: double (nullable = true)\n",
      " |-- price_x_satisfaction: double (nullable = true)\n",
      " |-- price_per_dist_km: double (nullable = true)\n",
      " |-- log_metro_dist: double (nullable = true)\n",
      " |-- log_bookings: double (nullable = true)\n",
      " |-- segment_listing_count: long (nullable = true)\n",
      " |-- segment_avg_price: double (nullable = true)\n",
      " |-- segment_median_price: double (nullable = true)\n",
      " |-- segment_p90_price: double (nullable = true)\n",
      " |-- segment_p10_price: double (nullable = true)\n",
      " |-- segment_price_std: double (nullable = true)\n",
      " |-- segment_price_spread: double (nullable = true)\n",
      " |-- price_vs_segment_median: double (nullable = true)\n",
      " |-- price_vs_segment_avg: double (nullable = true)\n",
      " |-- relative_price_volatility: double (nullable = true)\n",
      " |-- elasticity_slope: double (nullable = true)\n",
      "\n",
      "+-------+-----------------+--------------------+\n",
      "|summary|n_bookings       |elasticity_slope    |\n",
      "+-------+-----------------+--------------------+\n",
      "|count  |51707            |51707               |\n",
      "|mean   |13.88374881544085|-0.39296737455702824|\n",
      "|stddev |9.128371241965922|0.18419248037270775 |\n",
      "|min    |1                |-0.688012798321685  |\n",
      "|25%    |7                |-0.5420050880829435 |\n",
      "|50%    |11               |-0.43343679623243975|\n",
      "|75%    |19               |-0.277879548809525  |\n",
      "|max    |70               |0.10654104605998937 |\n",
      "+-------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load listing-level engineered features (Spark-only)\n",
    "listing_features_path = str(out_processed / 'listing_features.parquet')\n",
    "if not Path(listing_features_path).exists():\n",
    "    raise FileNotFoundError(f\"Missing {listing_features_path}. Run Notebook 02 first.\")\n",
    "\n",
    "df = (\n",
    "    spark.read.option('mergeSchema', 'true')\n",
    "    .parquet(listing_features_path)\n",
    "    .persist(StorageLevel.MEMORY_AND_DISK)\n",
    " )\n",
    "\n",
    "print(f\"Loaded listing_features.parquet with {len(df.columns)} columns\")\n",
    "df.printSchema()\n",
    "\n",
    "# Quick sanity checks\n",
    "required_cols = ['n_bookings', 'elasticity_slope']\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns in parquet: {missing}\")\n",
    "\n",
    "df.select(required_cols).summary().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84b2e476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['city',\n",
       " 'weekend',\n",
       " 'cat_city_amsterdam',\n",
       " 'cat_city_athens',\n",
       " 'cat_city_barcelona',\n",
       " 'cat_city_berlin',\n",
       " 'cat_city_budapest',\n",
       " 'cat_city_lisbon',\n",
       " 'cat_city_london',\n",
       " 'cat_city_paris',\n",
       " 'cat_city_rome',\n",
       " 'cat_city_vienna',\n",
       " 'is_weekend',\n",
       " 'listing_price',\n",
       " 'room_shared',\n",
       " 'room_private',\n",
       " 'person_capacity',\n",
       " 'host_is_superhost',\n",
       " 'multi',\n",
       " 'biz',\n",
       " 'cleanliness_rating',\n",
       " 'guest_satisfaction_overall',\n",
       " 'bedrooms',\n",
       " 'city_center_dist',\n",
       " 'metro_dist',\n",
       " 'n_bookings',\n",
       " 'cat_room_type_entire_home_apt',\n",
       " 'cat_room_type_private_room',\n",
       " 'cat_room_type_shared_room',\n",
       " 'log_price',\n",
       " 'price_per_person',\n",
       " 'price_per_bedroom',\n",
       " 'capacity_bin',\n",
       " 'quality_score',\n",
       " 'price_x_satisfaction',\n",
       " 'price_per_dist_km',\n",
       " 'log_metro_dist',\n",
       " 'log_bookings',\n",
       " 'segment_listing_count',\n",
       " 'segment_avg_price',\n",
       " 'segment_median_price',\n",
       " 'segment_p90_price',\n",
       " 'segment_p10_price',\n",
       " 'segment_price_std',\n",
       " 'segment_price_spread',\n",
       " 'price_vs_segment_median',\n",
       " 'price_vs_segment_avg',\n",
       " 'relative_price_volatility',\n",
       " 'elasticity_slope']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76b8777d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns: ['city', 'weekend', 'room_shared', 'room_private', 'log_bookings']\n",
      "Rows after label cleaning (lazy):\n",
      "+-----+\n",
      "| rows|\n",
      "+-----+\n",
      "|51707|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic cleaning: ensure label is double, and keep only usable feature columns\n",
    "from pyspark.sql.types import (\n",
    "    StringType, BooleanType, NumericType\n",
    " )\n",
    "\n",
    "target_col = 'n_bookings'\n",
    "label_col = 'label'\n",
    "\n",
    "df2 = df.withColumn(label_col, F.col(target_col).cast('double'))\n",
    "\n",
    "# Cast booleans to ints (Spark ML prefers numeric features)\n",
    "for field in df2.schema.fields:\n",
    "    if isinstance(field.dataType, BooleanType):\n",
    "        df2 = df2.withColumn(field.name, F.col(field.name).cast('int'))\n",
    "\n",
    "# Drop redundant columns if they exist\n",
    "_redundant = ['city', 'weekend', 'room_shared', 'room_private', 'log_bookings']\n",
    "_to_drop = [c for c in _redundant if c in df2.columns]\n",
    "if _to_drop:\n",
    "    df2 = df2.drop(*_to_drop)\n",
    "    print(f\"Dropped columns: {_to_drop}\")\n",
    "\n",
    "# Drop rows with null label\n",
    "df2 = df2.filter(F.col(label_col).isNotNull())\n",
    "\n",
    "print('Rows after label cleaning (lazy):')\n",
    "df2.select(F.count('*').alias('rows')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47bf6cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String feature cols: 1\n",
      "Numeric feature cols: 42\n",
      "Example string cols: ['capacity_bin']\n",
      "Example numeric cols: ['cat_city_amsterdam', 'cat_city_athens', 'cat_city_barcelona', 'cat_city_berlin', 'cat_city_budapest', 'cat_city_lisbon', 'cat_city_london', 'cat_city_paris', 'cat_city_rome', 'cat_city_vienna']\n",
      "Pipeline stages ready.\n"
     ]
    }
   ],
   "source": [
    "# Build a Spark ML pipeline (handles numeric + string cols)\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    Imputer, StringIndexer, OneHotEncoder, VectorAssembler\n",
    " )\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Columns to exclude from features\n",
    "exclude_cols = {target_col, label_col}\n",
    "for maybe_id in ['id', 'listing_id']:\n",
    "    if maybe_id in df2.columns:\n",
    "        exclude_cols.add(maybe_id)\n",
    "\n",
    "string_cols = []\n",
    "numeric_cols = []\n",
    "for field in df2.schema.fields:\n",
    "    if field.name in exclude_cols:\n",
    "        continue\n",
    "    if isinstance(field.dataType, StringType):\n",
    "        string_cols.append(field.name)\n",
    "    elif isinstance(field.dataType, NumericType):\n",
    "        numeric_cols.append(field.name)\n",
    "\n",
    "print(f\"String feature cols: {len(string_cols)}\")\n",
    "print(f\"Numeric feature cols: {len(numeric_cols)}\")\n",
    "if len(string_cols) > 0:\n",
    "    print('Example string cols:', string_cols[:10])\n",
    "print('Example numeric cols:', numeric_cols[:10])\n",
    "\n",
    "# Split\n",
    "train_df, test_df = df2.randomSplit([0.8, 0.2], seed=42)\n",
    "train_df = train_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "test_df = test_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Stages: index + encode for strings, impute for numerics, assemble features\n",
    "stages_common = []\n",
    "\n",
    "indexed_cols = [f\"{c}__idx\" for c in string_cols]\n",
    "ohe_cols = [f\"{c}__ohe\" for c in string_cols]\n",
    "for c, idx in zip(string_cols, indexed_cols):\n",
    "    stages_common.append(StringIndexer(inputCol=c, outputCol=idx, handleInvalid='keep'))\n",
    "if string_cols:\n",
    "    stages_common.append(OneHotEncoder(inputCols=indexed_cols, outputCols=ohe_cols, handleInvalid='keep'))\n",
    "\n",
    "imputed_numeric_cols = [f\"{c}__imp\" for c in numeric_cols]\n",
    "if numeric_cols:\n",
    "    stages_common.append(Imputer(inputCols=numeric_cols, outputCols=imputed_numeric_cols, strategy='median'))\n",
    "\n",
    "feature_cols = []\n",
    "feature_cols.extend(imputed_numeric_cols)\n",
    "feature_cols.extend(ohe_cols)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features', handleInvalid='keep')\n",
    "stages_common.append(assembler)\n",
    "\n",
    "# Evaluators\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=label_col, predictionCol='prediction', metricName='rmse')\n",
    "evaluator_mae = RegressionEvaluator(labelCol=label_col, predictionCol='prediction', metricName='mae')\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=label_col, predictionCol='prediction', metricName='r2')\n",
    "\n",
    "print('Pipeline stages ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ffbc7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LinearRegression...\n",
      "  RMSE=3.9581 | MAE=2.8923 | R2=0.8126\n",
      "Training GBTRegressor...\n",
      "  RMSE=3.5081 | MAE=2.4404 | R2=0.8528\n",
      "Training RandomForest...\n",
      "  RMSE=3.7543 | MAE=2.6411 | R2=0.8314\n",
      "\n",
      "Model comparison (sorted by RMSE):\n",
      "- GBTRegressor: RMSE=3.5081 | MAE=2.4404 | R2=0.8528\n",
      "- RandomForest: RMSE=3.7543 | MAE=2.6411 | R2=0.8314\n",
      "- LinearRegression: RMSE=3.9581 | MAE=2.8923 | R2=0.8126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'model': 'GBTRegressor',\n",
       "  'rmse': 3.5080533376905705,\n",
       "  'mae': 2.44036625075162,\n",
       "  'r2': 0.8527987728557824},\n",
       " {'model': 'RandomForest',\n",
       "  'rmse': 3.754254425849336,\n",
       "  'mae': 2.6410885734848866,\n",
       "  'r2': 0.831412078030317},\n",
       " {'model': 'LinearRegression',\n",
       "  'rmse': 3.958121125413672,\n",
       "  'mae': 2.8922900050809393,\n",
       "  'r2': 0.8126053382269172}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and compare Spark MLlib models (safe defaults for local Windows)\n",
    "def fit_eval(model_name: str, regressor):\n",
    "    pipeline = Pipeline(stages=stages_common + [regressor])\n",
    "    model = pipeline.fit(train_df)\n",
    "    preds = model.transform(test_df)\n",
    "    rmse = evaluator_rmse.evaluate(preds)\n",
    "    mae = evaluator_mae.evaluate(preds)\n",
    "    r2 = evaluator_r2.evaluate(preds)\n",
    "    return model, {'model': model_name, 'rmse': float(rmse), 'mae': float(mae), 'r2': float(r2)}\n",
    "\n",
    "# Start with lighter models; scale up once stable\n",
    "candidates = [\n",
    "    ('LinearRegression', LinearRegression(featuresCol='features', labelCol=label_col, regParam=0.0, elasticNetParam=0.0, maxIter=50)),\n",
    "    ('GBTRegressor', GBTRegressor(featuresCol='features', labelCol=label_col, maxIter=80, maxDepth=5, stepSize=0.1, subsamplingRate=0.8, seed=42)),\n",
    "    # RandomForest can be heavier on some Windows setups; keep it last and modest\n",
    "    ('RandomForest', RandomForestRegressor(featuresCol='features', labelCol=label_col, numTrees=50, maxDepth=8, maxBins=64, subsamplingRate=0.8, featureSubsetStrategy='sqrt', seed=42)),\n",
    " ]\n",
    "\n",
    "results = []\n",
    "trained = {}\n",
    "for name, reg in candidates:\n",
    "    print(f\"Training {name}...\")\n",
    "    m, r = fit_eval(name, reg)\n",
    "    trained[name] = m\n",
    "    results.append(r)\n",
    "    print(f\"  RMSE={r['rmse']:.4f} | MAE={r['mae']:.4f} | R2={r['r2']:.4f}\")\n",
    "\n",
    "results_sorted = sorted(results, key=lambda x: x['rmse'])\n",
    "print('')\n",
    "print('Model comparison (sorted by RMSE):')\n",
    "for r in results_sorted:\n",
    "    print(f\"- {r['model']}: RMSE={r['rmse']:.4f} | MAE={r['mae']:.4f} | R2={r['r2']:.4f}\")\n",
    "\n",
    "results_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c546644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model by RMSE: GBTRegressor\n",
      "Saved Spark PipelineModel to: c:\\Users\\Andres\\Documents\\Jupyter\\github portfolio\\PySpark LTSM\\airbnb\\models\\spark_best_model\n",
      "Saved metrics to: c:\\Users\\Andres\\Documents\\Jupyter\\github portfolio\\PySpark LTSM\\airbnb\\outputs\\spark_model_results.csv\n",
      "Saved feature importances to: c:\\Users\\Andres\\Documents\\Jupyter\\github portfolio\\PySpark LTSM\\airbnb\\outputs\\spark_feature_importance.csv\n",
      "Top 25 features:\n",
      "- is_weekend__imp: 0.362807\n",
      "- listing_price__imp: 0.195027\n",
      "- price_per_person__imp: 0.086064\n",
      "- host_is_superhost__imp: 0.081060\n",
      "- cleanliness_rating__imp: 0.072426\n",
      "- guest_satisfaction_overall__imp: 0.063346\n",
      "- quality_score__imp: 0.021954\n",
      "- price_per_bedroom__imp: 0.018370\n",
      "- price_x_satisfaction__imp: 0.012651\n",
      "- metro_dist__imp: 0.012297\n",
      "- elasticity_slope__imp: 0.011111\n",
      "- city_center_dist__imp: 0.010720\n",
      "- price_per_dist_km__imp: 0.008504\n",
      "- price_vs_segment_median__imp: 0.007070\n",
      "- price_vs_segment_avg__imp: 0.006178\n",
      "- segment_price_std__imp: 0.004302\n",
      "- relative_price_volatility__imp: 0.003666\n",
      "- person_capacity__imp: 0.003307\n",
      "- segment_avg_price__imp: 0.002340\n",
      "- bedrooms__imp: 0.002308\n",
      "- segment_listing_count__imp: 0.002302\n",
      "- segment_price_spread__imp: 0.001487\n",
      "- multi__imp: 0.001002\n",
      "- segment_median_price__imp: 0.000844\n",
      "- biz__imp: 0.000681\n"
     ]
    }
   ],
   "source": [
    "# Save best model + export metrics + feature importance (driver-side, no Spark createDataFrame)\n",
    "import csv\n",
    "\n",
    "best_name = results_sorted[0]['model']\n",
    "best_model = trained[best_name]\n",
    "print(f\"Best model by RMSE: {best_name}\")\n",
    "\n",
    "models_dir = project_root / 'models'\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "best_path = models_dir / 'spark_best_model'\n",
    "best_model.write().overwrite().save(str(best_path))\n",
    "print(f\"Saved Spark PipelineModel to: {best_path}\")\n",
    "\n",
    "# Save metrics (driver-side CSV)\n",
    "outputs_dir = project_root / 'outputs'\n",
    "outputs_dir.mkdir(parents=True, exist_ok=True)\n",
    "metrics_path = outputs_dir / 'spark_model_results.csv'\n",
    "with open(metrics_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=['model', 'rmse', 'mae', 'r2'])\n",
    "    w.writeheader()\n",
    "    for r in results_sorted:\n",
    "        w.writerow(r)\n",
    "print(f\"Saved metrics to: {metrics_path}\")\n",
    "\n",
    "# Feature importance (only for tree models)\n",
    "from pyspark.ml.regression import RandomForestRegressionModel, GBTRegressionModel\n",
    "\n",
    "assembler_stage = [s for s in best_model.stages if s.__class__.__name__ == 'VectorAssembler'][0]\n",
    "assembled_inputs = assembler_stage.getInputCols()\n",
    "last_stage = best_model.stages[-1]\n",
    "\n",
    "if isinstance(last_stage, (RandomForestRegressionModel, GBTRegressionModel)):\n",
    "    fi = last_stage.featureImportances\n",
    "    rows = [(assembled_inputs[i], float(fi[i])) for i in range(len(assembled_inputs))]\n",
    "    rows = sorted(rows, key=lambda t: t[1], reverse=True)\n",
    "    fi_path = outputs_dir / 'spark_feature_importance.csv'\n",
    "    with open(fi_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(['feature', 'importance'])\n",
    "        w.writerows(rows)\n",
    "    print(f\"Saved feature importances to: {fi_path}\")\n",
    "    print('Top 25 features:')\n",
    "    for feat, imp in rows[:25]:\n",
    "        print(f\"- {feat}: {imp:.6f}\")\n",
    "else:\n",
    "    print('Best model does not provide feature importances (expected for LinearRegression).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8e7c95",
   "metadata": {},
   "source": [
    "## Test Deep Learning\n",
    "\n",
    "Use PyTorch to train on a sample collected from Spark.\n",
    "\n",
    "- Uses the already-engineered features (including `elasticity_slope`)\n",
    "- Trains only a few epochs to test API usage before scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96c89320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo sample shapes: (8000, 47) (8000,)\n",
      "Feature dim: 47\n"
     ]
    }
   ],
   "source": [
    "# Create a small training sample from Spark for DL demos\n",
    "# (keep it small so it runs fast on CPU)\n",
    "\n",
    "# Reuse the same preprocessing stages to build a features vector, then collect a sample\n",
    "demo_regressor = LinearRegression(featuresCol='features', labelCol=label_col, maxIter=1)\n",
    "demo_pipeline = Pipeline(stages=stages_common + [demo_regressor])\n",
    "demo_model = demo_pipeline.fit(train_df.limit(1000))  # quick fit to materialize transformers\n",
    "demo_features_df = demo_model.transform(train_df).select('features', label_col)\n",
    "\n",
    "# Sample to the driver\n",
    "demo_n = 8000\n",
    "sample_df = demo_features_df.orderBy(F.rand(seed=42)).limit(demo_n)\n",
    "rows = sample_df.collect()\n",
    "X = np.vstack([r['features'].toArray() for r in rows]).astype('float32')\n",
    "y = np.array([r[label_col] for r in rows], dtype='float32')\n",
    "\n",
    "# Light target transform (optional, often helps neural nets for count-like targets)\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "print('Demo sample shapes:', X.shape, y.shape)\n",
    "print('Feature dim:', X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd197f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch epoch 1/5 | train_mse=417.5935 | val_mse=104.5468\n",
      "PyTorch epoch 2/5 | train_mse=25.1397 | val_mse=12.3923\n",
      "PyTorch epoch 3/5 | train_mse=5.7534 | val_mse=3.6118\n",
      "PyTorch epoch 4/5 | train_mse=2.0758 | val_mse=2.3189\n",
      "PyTorch epoch 5/5 | train_mse=1.4751 | val_mse=2.5375\n",
      "PyTorch demo RMSE (raw bookings): 5525.016\n"
     ]
    }
   ],
   "source": [
    "# --- PyTorch: MLP regressor (showcase) ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_log, test_size=0.2, random_state=42)\n",
    "X_train_t = torch.from_numpy(X_train)\n",
    "y_train_t = torch.from_numpy(y_train).view(-1, 1)\n",
    "X_val_t = torch.from_numpy(X_val)\n",
    "y_val_t = torch.from_numpy(y_val).view(-1, 1)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=256, shuffle=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(X.shape[1], 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1),\n",
    ")\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        opt.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total += float(loss) * len(xb)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = model(X_val_t)\n",
    "        val_loss = float(loss_fn(val_pred, y_val_t))\n",
    "    print(f\"PyTorch epoch {epoch+1}/5 | train_mse={total/len(X_train):.4f} | val_mse={val_loss:.4f}\")\n",
    "\n",
    "# Convert back from log1p for a quick, human-readable metric\n",
    "with torch.no_grad():\n",
    "    yhat_val = torch.expm1(model(X_val_t)).squeeze().numpy()\n",
    "y_val_raw = np.expm1(y_val)\n",
    "rmse = float(np.sqrt(np.mean((yhat_val - y_val_raw) ** 2)))\n",
    "print(f\"PyTorch demo RMSE (raw bookings): {rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b7c6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
